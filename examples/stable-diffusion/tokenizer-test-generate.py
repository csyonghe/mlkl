#!/usr/bin/env python3
"""
Generate CLIP tokenizer test data and download vocabulary/merges files.

This script:
1. Downloads the CLIP tokenizer vocabulary and merges files
2. Generates test cases comparing Python tokenizer output
"""

import os
import json
from pathlib import Path

# Try to import transformers for the reference tokenizer
try:
    from transformers import CLIPTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    print("Warning: transformers not installed. Run: pip install transformers")
    HAS_TRANSFORMERS = False

def get_script_dir():
    return Path(__file__).parent.resolve()

def download_tokenizer_files():
    """Download vocab.json and merges.txt from HuggingFace."""
    model_dir = get_script_dir() / "model"
    model_dir.mkdir(exist_ok=True)
    
    vocab_path = model_dir / "vocab.json"
    merges_path = model_dir / "merges.txt"
    
    if vocab_path.exists() and merges_path.exists():
        print(f"Tokenizer files already exist in {model_dir}")
        return vocab_path, merges_path
    
    print("Downloading tokenizer files from HuggingFace...")
    
    try:
        from huggingface_hub import hf_hub_download
        
        # Download from openai/clip-vit-large-patch14 (same as SD 1.5)
        repo_id = "openai/clip-vit-large-patch14"
        
        if not vocab_path.exists():
            downloaded = hf_hub_download(repo_id=repo_id, filename="vocab.json")
            import shutil
            shutil.copy(downloaded, vocab_path)
            print(f"Downloaded vocab.json to {vocab_path}")
        
        if not merges_path.exists():
            downloaded = hf_hub_download(repo_id=repo_id, filename="merges.txt")
            import shutil
            shutil.copy(downloaded, merges_path)
            print(f"Downloaded merges.txt to {merges_path}")
            
    except ImportError:
        print("Error: huggingface_hub not installed. Run: pip install huggingface_hub")
        print("Or manually download vocab.json and merges.txt from:")
        print("  https://huggingface.co/openai/clip-vit-large-patch14")
        return None, None
    except Exception as e:
        print(f"Error downloading tokenizer files: {e}")
        return None, None
    
    return vocab_path, merges_path

def generate_test_cases():
    """Generate test cases using the Python tokenizer."""
    if not HAS_TRANSFORMERS:
        print("Cannot generate test cases without transformers library")
        return
    
    # Load tokenizer
    print("Loading CLIP tokenizer...")
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
    
    # Test prompts - various edge cases
    test_prompts = [
        # Basic cases
        "",
        "hello",
        "Hello World",
        "a photo of a cat",
        "a painting of a dog",
        
        # Stable Diffusion style prompts
        "a beautiful landscape, digital art, trending on artstation",
        "portrait of a woman, highly detailed, 8k, photorealistic",
        "cyberpunk city at night, neon lights, rain",
        "fantasy castle on a mountain, epic, cinematic lighting",
        
        # Edge cases
        "123",
        "hello123world",
        "don't won't can't",
        "it's a test",
        
        # Punctuation
        "Hello, World!",
        "What? Why! How...",
        "test@email.com",
        
        # Special characters
        "cafÃ© rÃ©sumÃ© naÃ¯ve",
        "æ—¥æœ¬èªž",  # Japanese (should handle gracefully)
        "emoji ðŸŽ¨",  # Emoji
        
        # Long text (should truncate)
        "This is a very long prompt that goes on and on and contains many words " * 5,
        
        # Whitespace
        "  hello  world  ",
        "hello\nworld",
        "hello\tworld",
        
        # Numbers and mixed
        "2023 new year",
        "version 2.0",
        "50% off",
    ]
    
    # Create test data directory
    test_dir = get_script_dir() / "test_data"
    test_dir.mkdir(exist_ok=True)
    
    output_path = test_dir / "tokenizer_test_cases.txt"
    
    print(f"Generating {len(test_prompts)} test cases...")
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("# CLIP Tokenizer Test Cases\n")
        f.write("# Format: TEXT<TAB>TOKEN_ID,TOKEN_ID,...\n")
        f.write("# Generated by tokenizer-test-generate.py\n")
        f.write("#\n")
        
        for prompt in test_prompts:
            # Tokenize with padding to 77
            tokens = tokenizer(
                prompt,
                padding="max_length",
                max_length=77,
                truncation=True,
                return_tensors=None
            )
            token_ids = tokens["input_ids"]
            
            # Escape tabs and newlines in prompt
            escaped_prompt = prompt.replace("\t", "\\t").replace("\n", "\\n")
            
            # Write test case
            token_str = ",".join(str(t) for t in token_ids)
            f.write(f"{escaped_prompt}\t{token_str}\n")
            
            # Print first few for verification
            if len(test_prompts) <= 10 or test_prompts.index(prompt) < 3:
                print(f"  '{prompt[:50]}...' -> {token_ids[:5]}...")
    
    print(f"Wrote test cases to {output_path}")
    
    # Also print some statistics
    print("\nTokenizer info:")
    print(f"  Vocab size: {tokenizer.vocab_size}")
    print(f"  Start token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
    print(f"  End token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    print(f"  Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")

def verify_tokenizer_files():
    """Verify the tokenizer files are valid."""
    model_dir = get_script_dir() / "model"
    vocab_path = model_dir / "vocab.json"
    merges_path = model_dir / "merges.txt"
    
    if not vocab_path.exists():
        print(f"Error: vocab.json not found at {vocab_path}")
        return False
    
    if not merges_path.exists():
        print(f"Error: merges.txt not found at {merges_path}")
        return False
    
    # Check vocab
    with open(vocab_path, "r", encoding="utf-8") as f:
        vocab = json.load(f)
    print(f"Vocab size: {len(vocab)}")
    
    # Check for special tokens
    if "<|startoftext|>" not in vocab:
        print("Warning: <|startoftext|> not in vocab")
    if "<|endoftext|>" not in vocab:
        print("Warning: <|endoftext|> not in vocab")
    
    # Check merges
    with open(merges_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    # Count non-header lines
    merge_count = sum(1 for line in lines if line.strip() and not line.startswith("#"))
    print(f"Merge count: {merge_count}")
    
    return True

def main():
    print("=== CLIP Tokenizer Test Data Generator ===\n")
    
    # Download tokenizer files
    vocab_path, merges_path = download_tokenizer_files()
    if vocab_path is None:
        print("\nPlease download tokenizer files manually and re-run.")
        return
    
    # Verify files
    if not verify_tokenizer_files():
        return
    
    # Generate test cases
    if HAS_TRANSFORMERS:
        generate_test_cases()
    else:
        print("\nSkipping test case generation (transformers not installed)")
    
    print("\nDone!")

if __name__ == "__main__":
    main()
